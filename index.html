<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Parallel-R1: Towards Parallel Thinking via Reinforcement Learning">
  <meta name="keywords" content="Parallel Thinking, Reinforcement Learning, LLM, Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  
  <style>
    .hero.teaser .container {
      max-width: 100%;
    }
    .publication-title {
      font-family: 'Google Sans', sans-serif;
      font-weight: 500;
    }
    .publication-authors {
      font-family: 'Google Sans', sans-serif;
    }
    .publication-venue {
        color: #555;
        font-size: 1rem;
        font-weight: 500;
    }
    .link-block a {
      color: #3273dc;
    }
    .section {
      padding: 3rem 1.5rem;
    }
    .content h1, .content h2, .content h3 {
        font-family: 'Google Sans', sans-serif;
        font-weight: 500;
    }
    .publication-links .button {
        font-weight: 500;
    }
    .results-table {
        width: 100%;
        border-collapse: collapse;
        margin: 2rem 0;
        font-size: 0.9em;
    }
    .results-table th, .results-table td {
        border: 1px solid #dbdbdb;
        padding: 0.75em;
        vertical-align: middle;
    }
    .results-table thead {
        background-color: #f5f5f5;
        font-weight: bold;
    }
    .highlight-best {
        font-weight: bold;
        color: #257942;
    }
    .dnerf {
      color: #3273dc;
    }
    .case-study-container {
        display: flex;
        gap: 2rem;
    }
    .case-study {
        flex: 1;
    }
    .case-study img {
        border: 1px solid #ddd;
        border-radius: 5px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    /* Styles for the collapsible section */
    .collapsible-content {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.5s ease-in-out, margin-top 0.5s ease-in-out;
        margin-top: 0 !important;
    }
    .collapsible-content.is-active {
        max-height: 1000px; /* Large enough to fit content */
        margin-top: 2rem !important;
    }
    @media screen and (max-width: 768px) {
        .case-study-container {
            flex-direction: column;
        }
    }
  </style>
  
  <link rel="icon" href="data:;base64,iVBORw0KGgo=">

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</h1>
          <h3 class="subtitle is-4 publication-venue">Technical Report, 2025</h3>
          <div class="is-size-5 publication-authors">
             <span class="author-block">
              <a href="#">Tong Zheng</a><sup>1,2,â€ </sup>,</span>
            <span class="author-block">
              <a href="#">Hongming Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Wenhao Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Xiaoyang Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Xinyu Yang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="#">Runpeng Dai</a><sup>1,4</sup>,
            </span><br>
            <span class="author-block">
              <a href="#">Rui Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Huiwen Bao</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="#">Chengsong Huang</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="#">Heng Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Dong Yu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tencent AI Lab Seattle,</span>
            <span class="author-block"><sup>2</sup>University of Maryland, College Park,</span>
            <span class="author-block"><sup>3</sup>Carnegie Mellon University,</span><br>
            <span class="author-block"><sup>4</sup>University of North Carolina at Chapel Hill,</span>
            <span class="author-block"><sup>5</sup>City University of Hong Kong,</span>
            <span class="author-block"><sup>6</sup>Washington University in St. Louis</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="your_paper_link.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/your-arxiv-id"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg style="width:18px;height:18px;margin-top:-2px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg"><path d="M512 0C229.2224 0 0 229.2224 0 512s229.2224 512 512 512 512-229.2224 512-512S794.7776 0 512 0z m233.472 696.32L623.36 574.336l-92.16 122.368-124.416-165.888-122.368 165.888L162.816 328.192h142.336l91.648 122.368 124.416 165.888 122.368-165.888 91.136-122.368h142.336L745.472 696.32z" fill="#FFFFFF"></path></svg>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/zhengkid/Parallel-R1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="https://i.imgur.com/vHq4gYd.png" alt="An overview of the Parallel-R1 framework" style="width: 100%; border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);">
      <h2 class="subtitle has-text-centered" style="margin-top: 2rem;">
        <span class="dnerf">Parallel-R1</span> teaches Large Language Models to learn parallel thinking through reinforcement learning, turning a single line of thought into a multi-path reasoning strategy.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization.
          </p>
          <p>
            Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks show that Parallel-R1 successfully instills parallel thinking, leading to significant accuracy improvements over sequential thinking models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" style="background-color: #f9f9f9;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
      </div>
    </div>
    
    <div class="columns is-centered">
        <div class="column">
            <h3 class="title is-4 has-text-centered">Progressive Curriculum</h3>
            <img src="https://i.imgur.com/eBf2K5t.png" alt="Method Diagram" style="width: 100%; height: auto; display: block; margin: 0 auto; border-radius:10px;">
            <div class="content has-text-justified" style="margin-top: 2rem;">
              <p>
                LLMs cannot learn parallel thinking with RL from scratch because they have not seen such behavior during pre-training. To solve this "cold-start" problem, we propose a progressive, multi-stage training approach.
              </p>
              <ul>
                <li><strong>Stage 1: Cold-Start SFT on Easy Math</strong>: We begin with SFT on simpler problems. We found that high-quality parallel thinking data for easy tasks (like GSM8K) can be generated easily via simple prompting. This initial stage uses our created Parallel-GSM8K dataset to effectively teach the model the basic format of parallel thinking.</li>
                <li><strong>(Optional) Stage 2: Small-Scale RL</strong>: To stabilize format learning, we perform small-scale RL on the same easy question set, using a reward that encourages both accuracy and parallel structures.</li>
                <li><strong>Stage 3: Large-Scale RL on General Math</strong>: Finally, the model transitions to RL on more difficult tasks (like the DAPO dataset) to generalize this ability, using an accuracy-only reward.</li>
              </ul>
            </div>
        </div>
    </div>
    
    <div class="columns is-centered is-vcentered" style="margin-top: 4rem;">
        <div class="column is-half">
            <h3 class="title is-4">Model Variants: Causal vs. Structured</h3>
            <div class="content has-text-justified">
                <p>
                    We explore two settings for learning parallel thinking. The standard approach uses a normal <strong>Causal Model</strong> (termed Parallel-R1-Seen) without architectural changes.
                </p>
                <p>
                    We also introduce a <strong>Structured Variant</strong> (termed Parallel-R1-Unseen) which incorporates explicit inductive biases to enforce path isolation. This is achieved through:
                </p>
                 <ul>
                    <li><strong>Path-window masking</strong>: Restricts a token within a `<Path>` block to only attend to tokens from the same path and the shared context, preventing information leakage.</li>
                    <li><strong>Multiverse position encodings</strong>: Assigns disjoint sets of position indices to each path so their embeddings do not overlap.</li>
                </ul>
            </div>
        </div>
        <div class="column is-half">
            <img src="https://i.imgur.com/B9B1I1i.png" alt="Illustration of the structured attention mask and position IDs for the structured model variant.">
            <p class="is-size-7 has-text-centered" style="margin-top: 0.5rem;">Figure 2 from the paper: Structured attention mask and position IDs.</p>
        </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results & Key Findings</h2>
        </div>
      </div>

    <div class="columns is-centered">
        <div class="column">
            <h3 class="title is-4 has-text-centered">Main Results</h3>
            <p class="has-text-centered">
              Our progressive Parallel-R1 framework consistently outperforms strong baselines on mathematical reasoning benchmarks. The top-performing causal variant, <strong>Parallel-R1-Seen</strong>, achieves the highest average score of 48.9.
            </p>
            <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth results-table">
                <thead>
                    <tr>
                        <th rowspan="2">Method</th> <th rowspan="2">Parallel Ratio (%)</th> <th colspan="2">AIME25</th> <th colspan="2">AIME24</th> <th colspan="2">AMC23</th> <th rowspan="2">MATH (Mean@1)</th> <th rowspan="2">Avg.</th>
                    </tr>
                    <tr>
                        <th>Mean@16</th> <th>Pass@16</th> <th>Mean@16</th> <th>Pass@16</th> <th>Mean@16</th> <th>Pass@16</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="background-color: #fafafa;"><td colspan="9"><strong>Baselines</strong></td></tr>
                    <tr><td>GRPO (DAPO)</td><td>0.0</td><td>14.8</td><td>32.4</td><td>18.5</td><td>30.6</td><td>63.6</td><td>85.1</td><td>83.5</td><td>45.1</td></tr>
                    <tr><td>+ RL on GSM8K</td><td>0.0</td><td>13.3</td><td>26.3</td><td>18.8</td><td>34.9</td><td>66.4</td><td>82.2</td><td>82.6</td><td>45.3</td></tr>
                    <tr style="background-color: #fafafa;"><td colspan="9"><strong>Our Method</strong></td></tr>
                    <tr style="background-color: #e9f5ff;"><td><strong>Parallel-R1-Seen</strong></td><td>27.3</td><td class="highlight-best">19.2</td><td>38.9</td><td class="highlight-best">19.4</td><td>37.1</td><td class="highlight-best">70.5</td><td>85.0</td><td class="highlight-best">86.7</td><td class="highlight-best">48.9</td></tr>
                </tbody>
                <caption class="is-size-7" style="caption-side: bottom; text-align: center; margin-top: 0.5rem;">Table 2 from the paper: Performance comparison on mathematical reasoning benchmarks.</caption>
            </table>
        </div>
    </div>
    
    <div class="columns is-centered" style="margin-top: 4rem;">
        <div class="column">
            <h3 class="title is-4 has-text-centered">Ablation on Reward Modeling</h3>
             <p class="has-text-centered">
              To find how to effectively stimulate parallel thinking, we tested several reward strategies on our structured model. We found that an alternating approach provides the best balance between a high parallel ratio and strong performance.
            </p>
            <table class="table is-bordered is-fullwidth results-table">
                <thead>
                    <tr><th>Training Configuration</th><th>Parallel Ratio (%)</th><th>AIME 25</th><th>AIME 24</th><th>AMC 23</th><th>MATH</th></tr>
                </thead>
                <tbody>
                    <tr><td>Accuracy Only</td><td>13.6</td><td>17.7</td><td>18.3</td><td>69.7</td><td>82.6</td></tr>
                    <tr><td>Parallel Only</td><td>80.3</td><td>17.7</td><td>15.2</td><td>59.4</td><td>81.7</td></tr>
                    <tr style="background-color: #e9f5ff;"><td><strong>Alternating Acc./Parallel</strong></td><td class="highlight-best">63.0</td><td class="highlight-best">19.0</td><td>16.3</td><td>67.5</td><td class="highlight-best">84.5</td></tr>
                </tbody>
                <caption class="is-size-7" style="caption-side: bottom; text-align: center; margin-top: 0.5rem;">Table 4 from the paper: Ablation study on reward modeling for the Parallel-R1-Unseen model.</caption>
            </table>
        </div>
    </div>

    <div class="columns is-centered is-vcentered" style="margin-top: 4rem;">
        <div class="column is-half">
            <h3 class="title is-4">Finding: Evolution of Behavior</h3>
            <div class="content has-text-justified">
                <p>
                  Our analysis reveals a clear strategic evolution: the model initially leverages parallel paths for computational exploration, but as it gains proficiency, its strategy shifts towards using them for multi-perspective verification to confirm the final answer. This is shown by the relative position of the <code>&lt;Parallel&gt;</code> block moving later in the generation process as training progresses.
                </p>
                 <button class="button is-info is-outlined is-small" id="toggle-cases-btn" style="margin-top: 1rem;">Show Supporting Case Studies</button>
            </div>
        </div>
        <div class="column is-half">
            <img src="https://i.imgur.com/iC5uS9u.png" alt="Graph showing the relative position of the parallel block during RL training">
            <p class="is-size-7 has-text-centered" style="margin-top: 0.5rem;">Figure 3 from the paper: Positional dynamics of the <code>&lt;Parallel&gt;</code> block.</p>
        </div>
    </div>

    <div id="case-studies-collapsible" class="collapsible-content">
        <div class="columns is-centered">
            <div class="column is-four-fifths">
                <div class="case-study-container">
                    <div class="case-study">
                        <h4 class="title is-5 has-text-centered">Early Stage: Exploration</h4>
                        <img src="https://i.imgur.com/u15j6mC.png" alt="Case study of an early-stage model using parallel thinking for exploration.">
                        <p class="is-size-7 has-text-centered" style="margin-top: 0.5rem;">Figure 5 from the paper: An early-stage model explores two distinct algebraic methods to find possible answers.</p>
                    </div>
                    <div class="case-study">
                        <h4 class="title is-5 has-text-centered">Late Stage: Verification</h4>
                        <img src="https://i.imgur.com/N748v2L.png" alt="Case study of a late-stage model using parallel thinking for verification.">
                        <p class="is-size-7 has-text-centered" style="margin-top: 0.5rem;">Figure 6 from the paper: A late-stage model first derives a solution, then uses the parallel block to verify its conclusion.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="columns is-centered is-vcentered" style="margin-top: 4rem;">
        <div class="column is-half">
            <h3 class="title is-4">Finding: Parallel Thinking as a Mid-Training Scaffold</h3>
            <div class="content has-text-justified">
                <p>
                  We found that parallel thinking can serve as an effective structured exploration mechanism to improve RL training. By temporarily compelling the model to generate multiple, parallel thought blocks (Stage 1: Exploration), we introduce a strong inductive bias that forces a more diverse exploration, guiding the model toward more robust policy spaces.
                </p>
                <p>
                  When we later switch the training objective to optimize for accuracy alone (Stage 2: Exploitation), the model's performance improves significantly, reaching a peak AIME25 accuracy of 25.6%, surpassing the baseline. This suggests the value of parallel thinking lies not just in the structure itself, but in the robust policy space it helps discover.
                </p>
            </div>
        </div>
        <div class="column is-half">
            <img src="https://i.imgur.com/3YgD7Q1.png" alt="Graph showing two-stage training with parallel reasoning as a scaffold">
            <p class="is-size-7 has-text-centered" style="margin-top: 0.5rem;">Figure 4 from the paper: Two-stage training with parallel reasoning as a mid-training exploration scaffold.</p>
        </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zheng2025parallelr1,
      title={Parallel-R1: Towards Parallel Thinking via Reinforcement Learning}, 
      author={Tong Zheng and Hongming Zhang and Wenhao Yu and Xiaoyang Wang and Xinyu Yang and Runpeng Dai and Rui Liu and Huiwen Bao and Chengsong Huang and Heng Huang and Dong Yu},
      year={2025},
      eprint={your-arxiv-id},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage template was borrowed from the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  // Script for the collapsible case studies
  document.addEventListener('DOMContentLoaded', () => {
    const toggleBtn = document.getElementById('toggle-cases-btn');
    const content = document.getElementById('case-studies-collapsible');

    if (toggleBtn && content) {
      toggleBtn.addEventListener('click', () => {
        content.classList.toggle('is-active');
        if (content.classList.contains('is-active')) {
          toggleBtn.textContent = 'Hide Supporting Case Studies';
        } else {
          toggleBtn.textContent = 'Show Supporting Case Studies';
        }
      });
    }
  });
</script>

</body>
</html>
