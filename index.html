<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Parallel-R1: Towards Parallel Thinking via Reinforcement Learning">
  <meta name="keywords" content="Parallel Thinking, Reinforcement Learning, LLM, Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  
  <style>
    .hero.teaser .container {
      max-width: 100%;
    }
    .publication-title {
      font-family: 'Google Sans', sans-serif;
      font-weight: 500;
    }
    .publication-authors {
      font-family: 'Google Sans', sans-serif;
    }
    .publication-venue {
        color: #555;
        font-size: 1rem;
        font-weight: 500;
    }
    .link-block a {
      color: #3273dc;
    }
    .section {
      padding: 3rem 1.5rem;
    }
    .content h1, .content h2, .content h3 {
        font-family: 'Google Sans', sans-serif;
        font-weight: 500;
    }
    .publication-links .button {
        font-weight: 500;
    }
    .results-table {
        width: 100%;
        border-collapse: collapse;
        margin: 2rem 0;
    }
    .results-table th, .results-table td {
        border: 1px solid #dbdbdb;
        padding: 0.75em;
        vertical-align: middle;
    }
    .results-table thead {
        background-color: #f5f5f5;
        font-weight: bold;
    }
    .highlight-best {
        font-weight: bold;
        color: #257942;
    }
  </style>
  
  <link rel="icon" href="data:;base64,iVBORw0KGgo=">

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
           <h1 class="title is-1 publication-title">Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</h1>
          <h3 class="subtitle is-4 publication-venue">Technical Report</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kidzheng.github.io/">Tong Zheng</a><sup>1,2,â€ </sup>,</span>
            <span class="author-block">
              <a href="https://panda0881.github.io/Hongming_Homepage/">Hongming Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://wyu97.github.io/">Wenhao Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=EeppWmkAAAAJ&hl=en">Xiaoyang Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://xinyuyang.me/">Xinyu Yang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/runpeng-dai-453aa7195/">Runpeng Dai</a><sup>1,4</sup>,
            </span><br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=pMeiokwAAAAJ&hl=en">Rui Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=3PnRUyQAAAAJ">Huiwen Bao</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://chengsong-huang.github.io/">Chengsong Huang</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.umd.edu/~heng/">Heng Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/dongyu888/">Dong Yu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tencent AI Lab Seattle,</span>
            <span class="author-block"><sup>2</sup>University of Maryland, College Park,</span>
            <span class="author-block"><sup>3</sup>Carnegie Mellon University,</span>
            <span class="author-block"><sup>4</sup>University of North Carolina at Chapel Hill,</span><br>
            <span class="author-block"><sup>5</sup>City University of Hong Kong,</span>
             <span class="author-block"><sup>6</sup>Washington University in St. Louis</span> 
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/your-paper-id.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/your-paper-id"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/zhengkid/Parallel-R1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="https://i.imgur.com/vHq4gYd.png" alt="An overview of the Parallel-R1 framework" style="width: 100%; border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);">
      <h2 class="subtitle has-text-centered" style="margin-top: 2rem;">
         <span style="font-weight:bold; color:#3273DC;">Parallel-R1</span> teaches Large Language Models to learn parallel thinking through reinforcement learning, turning a single line of thought into a multi-path reasoning strategy. 
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
             Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently.  However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization.
          </p>
          <p>
             Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks.  Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL.  We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems.  Experiments on various math benchmarks show that Parallel-R1 successfully instills parallel thinking, leading to significant accuracy improvements over sequential thinking models.
          </p>
        </div>
      </div>
    </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method: A Progressive Curriculum</h2>
        <img src="https://i.imgur.com/eBf2K5t.png" alt="Method Diagram" style="width: 100%; height: auto; display: block; margin: 0 auto; border-radius:10px;">
        <div class="content has-text-justified" style="margin-top: 2rem;">
          <p>
             LLMs cannot learn parallel thinking with RL from scratch because they have not seen such behavior during pre-training and thus cannot generate such trajectories for the model to learn from.  To solve this "cold-start" problem, we propose a progressive, multi-stage training approach.
          </p>
          <ul>
            <li>
                 <strong>Stage 1: Cold-Start SFT on Easy Math</strong>: We begin with supervised fine-tuning on simpler problems.  We found that high-quality parallel thinking data for easy tasks (like GSM8K) can be generated easily via simple prompting.  This initial stage uses our created Parallel-GSM8K dataset to effectively teach the model the basic format of parallel thinking.
            </li>
            <li>
                 <strong>(Optional) Stage 2: Small-Scale RL</strong>: To enhance and stabilize the format learning, we can perform small-scale reinforcement learning on the same easy question set, using a reward that encourages both accuracy and the use of parallel structures.
            </li>
            <li>
                 <strong>Stage 3: Large-Scale RL on General Math</strong>: Finally, the model transitions to reinforcement learning on more difficult tasks (like the DAPO dataset) to explore and generalize this new ability.  At this stage, the primary goal is to improve task performance, so we use an accuracy-only reward.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Key Findings & Results</h2>
        </div>
      </div>

    <div class="columns is-centered">
        <div class="column">
            <h3 class="title is-4 has-text-centered">Main Results</h3>
            <p class="has-text-centered">
               Performance comparison on mathematical reasoning benchmarks for the Qwen-3-4B-Base model.  Our progressive Parallel-R1 framework consistently outperforms all baselines.  The top-performing causal variant, <strong>Parallel-R1-Seen</strong>, achieves the highest average score of 48.9.
            </p>
            <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth results-table">
                <thead>
                    <tr>
                        <th rowspan="2">Method</th>
                        <th rowspan="2">Parallel Ratio (%)</th>
                        <th colspan="2">AIME25</th>
                        <th colspan="2">AIME24</th>
                        <th colspan="2">AMC23</th>
                        <th rowspan="2">MATH (Mean@1)</th>
                        <th rowspan="2">Avg.</th>
                    </tr>
                    <tr>
                        <th>Mean@16</th>
                        <th>Pass@16</th>
                        <th>Mean@16</th>
                        <th>Pass@16</th>
                        <th>Mean@16</th>
                        <th>Pass@16</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td colspan="10" style="background-color: #f2f2f2; font-style: italic;"><strong>Baselines</strong></td>
                    </tr>
                    <tr>
                        <td>GRPO (DAPO)</td>
                         <td>0.0  </td>
                         <td>14.8  </td>
                         <td>32.4  </td>
                         <td>18.5  </td>
                         <td>30.6  </td>
                         <td>63.6  </td>
                         <td>85.1  </td>
                         <td>83.5  </td>
                         <td>45.1  </td>
                    </tr>
                    <tr>
                        <td>+ RL on GSM8K</td>
                         <td>0.0  </td>
                         <td>13.3  </td>
                         <td>26.3  </td>
                         <td>18.8  </td>
                         <td>34.9  </td>
                         <td>66.4  </td>
                         <td>82.2  </td>
                         <td>82.6  </td>
                         <td>45.3  </td>
                    </tr>
                    <tr>
                        <td colspan="10" style="background-color: #f2f2f2; font-style: italic;"><strong>Our Method</strong></td>
                    </tr>
                    <tr style="background-color: #e9f5ff;">
                        <td><strong>Parallel-R1-Seen</strong></td>
                         <td>27.3  </td>
                         <td class="highlight-best">19.2  </td>
                         <td>38.9  </td>
                         <td class="highlight-best">19.4  </td>
                         <td>37.1  </td>
                         <td class="highlight-best">70.5  </td>
                         <td>85.0  </td>
                         <td class="highlight-best">86.7  </td>
                         <td class="highlight-best">48.9  </td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
    
    <div class="columns is-centered is-vcentered" style="margin-top: 4rem;">
        <div class="column is-half">
            <h3 class="title is-4">Finding 1: Evolution of Behavior</h3>
            <div class="content has-text-justified">
                <p>
                   Our analysis reveals a clear strategic evolution: the model initially leverages parallel paths for computational exploration to discover potential solutions.  As it gains proficiency, its strategy shifts towards using them for multi-perspective verification to confirm the final answer .  This is evidenced by the relative position of the <code>&lt;Parallel&gt;</code> block moving later in the generation process as training progresses.
                </p>
            </div>
        </div>
        <div class="column is-half">
            <img src="https://i.imgur.com/iC5uS9u.png" alt="Graph showing the relative position of the parallel block during RL training">
        </div>
    </div>

    <div class="columns is-centered is-vcentered" style="margin-top: 4rem;">
        <div class="column is-half">
             <h3 class="title is-4">Finding 2: Parallel Thinking as an Exploration Scaffold</h3>
            <div class="content has-text-justified">
                <p>
                   We hypothesize that parallel thinking can serve as an effective structured exploration mechanism to improve RL training.  By forcing the model to generate multiple thought blocks, we introduce a strong inductive bias that guides it toward more robust policy spaces.  Our experiments validate this: after an initial exploration phase with forced parallelism, switching to an accuracy-only reward allows performance to continue improving, reaching a peak AIME25 accuracy of 25.6%, a notable improvement over the baseline GRPO model.
                </p>
            </div>
        </div>
        <div class="column is-half">
            <img src="https://i.imgur.com/3YgD7Q1.png" alt="Graph showing two-stage training with parallel reasoning as a scaffold">
        </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zheng2025parallelr1,
      title={Parallel-R1: Towards Parallel Thinking via Reinforcement Learning}, 
      author={Tong Zheng and Hongming Zhang and Wenhao Yu and Xiaoyang Wang and Xinyu Yang and Runpeng Dai and Rui Liu and Huiwen Bao and Chengsong Huang and Heng Huang and Dong Yu},
      year={2025},
      eprint={your-arxiv-id},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage template was borrowed from the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
