<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Parallel-R1: Towards Parallel Thinking via Reinforcement Learning">
  <meta name="keywords" content="Parallel Thinking, Reinforcement Learning, LLM, Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  
  <style>
    .hero.teaser .container {
      max-width: 100%;
    }
    .publication-title {
      font-family: 'Google Sans', sans-serif;
      font-weight: 500;
    }
    .publication-authors {
      font-family: 'Google Sans', sans-serif;
    }
    .publication-venue {
        color: #555;
        font-size: 1rem;
        font-weight: 500;
    }
    .link-block a {
      color: #3273dc;
    }
    .section {
      padding: 3rem 1.5rem;
    }
    .content h1, .content h2, .content h3, .content h4, .content h5 {
        font-family: 'Google Sans', sans-serif;
        font-weight: 500;
    }
    .publication-links .button {
        font-weight: 500;
    }
    .results-table {
        width: 100%;
        border-collapse: collapse;
        margin: 1.5rem 0;
        font-size: 0.85em;
    }
    .results-table th, .results-table td {
        border: 1px solid #dbdbdb;
        padding: 0.7em;
        vertical-align: middle;
        white-space: nowrap;
    }
    .results-table thead {
        background-color: #f5f5f5;
        font-weight: bold;
    }
    .highlight-best {
        font-weight: bold;
        color: #257942;
    }
    .dnerf {
      color: #3273dc;
    }
    .case-study-container {
        display: flex;
        gap: 2rem;
    }
    .case-study {
        flex: 1;
    }
    .case-study img {
        border: 1px solid #ddd;
        border-radius: 5px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    /* Styles for the collapsible section */
    .collapsible-content {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.5s ease-in-out, margin-top 0.5s ease-in-out;
        margin-top: 0 !important;
    }
    .collapsible-content.is-active {
        max-height: 1000px; /* Large enough to fit content */
        margin-top: 2rem !important;
    }
    @media screen and (max-width: 768px) {
        .case-study-container {
            flex-direction: column;
        }
    }
  </style>
  
  <link rel="icon" href="data:;base64,iVBORw0KGgo=">

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</h1>
          <h3 class="subtitle is-4 publication-venue">Technical Report, 2025</h3>
          <div class="is-size-5 publication-authors">
             <span class="author-block">
              <a href="#">Tong Zheng</a><sup>1,2,â€ </sup>,</span>
            <span class="author-block">
              <a href="#">Hongming Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Wenhao Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Xiaoyang Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Xinyu Yang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="#">Runpeng Dai</a><sup>1,4</sup>,
            </span><br>
            <span class="author-block">
              <a href="#">Rui Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Huiwen Bao</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="#">Chengsong Huang</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="#">Heng Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Dong Yu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tencent AI Lab Seattle,</span>
            <span class="author-block"><sup>2</sup>University of Maryland, College Park,</span>
            <span class="author-block"><sup>3</sup>Carnegie Mellon University,</span><br>
            <span class="author-block"><sup>4</sup>University of North Carolina at Chapel Hill,</span>
            <span class="author-block"><sup>5</sup>City University of Hong Kong,</span>
            <span class="author-block"><sup>6</sup>Washington University in St. Louis</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="your_paper_link.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/your-arxiv-id"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg style="width:18px;height:18px;margin-top:-2px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg"><path d="M512 0C229.2224 0 0 229.2224 0 512s229.2224 512 512 512 512-229.2224 512-512S794.7776 0 512 0z m233.472 696.32L623.36 574.336l-92.16 122.368-124.416-165.888-122.368 165.888L162.816 328.192h142.336l91.648 122.368 124.416 165.888 122.368-165.888 91.136-122.368h142.336L745.472 696.32z" fill="#FFFFFF"></path></svg>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/zhengkid/Parallel-R1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./static/images/Parallel_Thinking_illustration.jpg" alt="An overview of the Parallel-R1 framework" style="width: 100%; border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);">
      <h2 class="subtitle has-text-centered" style="margin-top: 2rem;">
        <span class="dnerf">Parallel-R1</span> teaches Large Language Models to learn parallel thinking through reinforcement learning, turning a single line of thought into a multi-path reasoning strategy.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization.
          </p>
          <p>
            Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks show that Parallel-R1 successfully instills parallel thinking, leading to significant accuracy improvements over sequential thinking models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" style="background-color: #f9f9f9;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The Core Challenge: The Data Bottleneck</h2>
      </div>
    </div>
    
    <div class="columns is-centered">
        <div class="column is-four-fifths">
            <div class="content has-text-justified">
                <p>
                    A significant challenge in teaching parallel thinking is collecting high-quality data, which is extremely rare in natural text distributions. Prior work relies on complex and costly data pipelines to synthesize such data.
                </p>
                <article class="message is-info">
                  <div class="message-body">
                    <strong>Key Finding</strong>: Our method is based on the discovery that it is easy to generate parallel thinking data for simple tasks, but extremely difficult for complex ones.
                  </div>
                </article>
                <p>
                    The table below illustrates this: a powerful model can generate valid parallel traces for 83.7% of simple GSM8K problems, but fails completely (0.0%) on challenging DAPO problems. Based on this, we created the <strong>Parallel-GSM8K</strong> dataset by using simple prompting on easy tasks. This dataset is strategically used only to teach the model the <em>format</em> of parallel thinking, which solves the data problem and enables our efficient reinforcement learning framework.
                </p>
            </div>
            <table class="table is-bordered is-fullwidth is-striped" style="margin-top:1rem;">
                <thead>
                    <tr><th>Data</th><th># Samples</th><th>Parallel Thinking Format (%)</th></tr>
                </thead>
                <tbody>
                    <tr><td>GSM8K</td><td>7,472</td><td>83.7</td></tr>
                    <tr><td>DAPO</td><td>17,916</td><td>0.0</td></tr>
                </tbody>
            </table>
            <p class="is-size-7 has-text-centered">Table 1 from the paper: Data quality comparison.</p>
        </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Our Solution: The Parallel-R1 Framework</h2>
            </div>
        </div>
        
        <div class="columns is-centered">
            <div class="column is-four-fifths">
                <h3 class="title is-4 has-text-centered">Progressive Curriculum</h3>
                <img src="./static/images/Progressive_curriculum.jpg" alt="Method Diagram" style="width: 100%; height: auto; display: block; margin: 0 auto; border-radius:10px;">
                <div class="content has-text-justified" style="margin-top: 2rem;">
                  <p>
                    To solve the "cold-start" problem, we propose a progressive, multi-stage training approach:
                  </p>
                  <ul>
                    <li><strong>Stage 1: Cold-Start SFT on Easy Math</strong>: We begin with SFT on our Parallel-GSM8K dataset to effectively teach the model the basic format of parallel thinking.</li>
                    <li><strong>(Optional) Stage 2: Small-Scale RL</strong>: To stabilize format learning, we perform small-scale RL on the same easy question set.</li>
                    <li><strong>Stage 3: Large-Scale RL on General Math</strong>: Finally, the model transitions to RL on more difficult tasks to generalize this ability.</li>
                  </ul>
                </div>
            </div>
        </div>
        
        <div class="columns is-centered is-vcentered" style="margin-top: 4rem;">
            <div class="column is-half">
                <h3 class="title is-4">Model Variants: Causal vs. Structured</h3>
                <div class="content has-text-justified">
                    <p>
                        We explore two settings. The standard approach uses a normal <strong>Causal Model</strong> (Parallel-R1-Seen). We also introduce a <strong>Structured Variant</strong> (Parallel-R1-Unseen) which incorporates explicit inductive biases to enforce path isolation through path-window masking and multiverse position encodings.
                    </p>
                </div>
            </div>
            <div class="column is-half">
                <img src="./static/images/model_architecture.jpg" alt="Illustration of the structured attention mask and position IDs.">
                <p class="is-size-7 has-text-centered" style="margin-top: 0.5rem;">Figure 2 from the paper: Structured attention mask.</p>
            </div>
        </div>
    </div>
</section>

<section class="section" style="background-color: #f9f9f9;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results and Analysis</h2>
        </div>
      </div>

    <div class="columns is-centered">
        <div class="column is-four-fifths">
            <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Main Results</h3>
            <article class="message is-info">
              <div class="message-body">
                Our progressive Parallel-R1 framework consistently outperforms all baselines. The top-performing causal variant, <strong>Parallel-R1-Seen</strong>, achieves an average score of 48.9 across four challenging math benchmarks, a significant improvement over the baseline GRPO model's 45.1.
              </div>
            </article>
            <div style="overflow-x: auto;">
                <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth results-table">
                    <thead>
                        <tr>
                            <th rowspan="2">Method</th> <th rowspan="2"># Parallel (%)</th> <th colspan="2">AIME25</th> <th colspan="2">AIME24</th> <th colspan="2">AMC23</th> <th rowspan="2">MATH (Mean@1)</th> <th rowspan="2">Avg.</th>
                        </tr>
                        <tr>
                            <th>Mean@16</th> <th>Pass@16</th> <th>Mean@16</th> <th>Pass@16</th> <th>Mean@16</th> <th>Pass@16</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>Qwen3-4B-Base</td><td>0.0</td><td>1.3</td><td>10.2</td><td>2.9</td><td>16.5</td><td>8.1</td><td>51.2</td><td>13.9</td><td>6.6</td></tr>
                        <tr style="background-color: #fafafa;"><td colspan="10" style="font-style: italic; text-align: left;"><strong>SFT + Parallel</strong></td></tr>
                        <tr><td>Parallel-SFT-Seen</td><td>95.6</td><td>8.0</td><td>29.8</td><td>10.6</td><td>26.4</td><td>48.9</td><td>79.2</td><td>76.6</td><td>36.0</td></tr>
                        <tr><td>Parallel-SFT-Unseen</td><td>95.6</td><td>5.2</td><td>20.9</td><td>8.5</td><td>26.7</td><td>41.7</td><td>80.1</td><td>71.5</td><td>31.7</td></tr>
                        <tr style="background-color: #fafafa;"><td colspan="10" style="font-style: italic; text-align: left;"><strong>RL Approach</strong></td></tr>
                        <tr><td>GRPO (DAPO)</td><td>0.0</td><td>14.8</td><td>32.4</td><td>18.5</td><td>30.6</td><td>63.6</td><td>85.1</td><td>83.5</td><td>45.1</td></tr>
                        <tr><td>&nbsp;&nbsp; + RL on GSM8K</td><td>0.0</td><td>13.3</td><td>26.3</td><td>18.8</td><td>34.9</td><td>66.4</td><td>82.2</td><td>82.6</td><td>45.3</td></tr>
                        <tr style="background-color: #e9f5ff;"><td>Parallel-R1-Seen</td><td>27.3</td><td class="highlight-best">19.2</td><td>38.9</td><td class="highlight-best">19.4</td><td class="highlight-best">37.1</td><td class="highlight-best">70.5</td><td>85.0</td><td class="highlight-best">86.7</td><td class="highlight-best">48.9</td></tr>
                        <tr><td>Parallel-R1-Unseen (S1)</td><td>13.6</td><td>17.7</td><td>37.8</td><td>18.3</td><td>33.2</td><td>69.7</td><td>88.9</td><td>82.6</td><td>47.1</td></tr>
                        <tr><td>Parallel-R1-Unseen (S2)</td><td class="highlight-best">63.0</td><td>19.0</td><td class="highlight-best">42.2</td><td>16.3</td><td>31.8</td><td>67.5</td><td class="highlight-best">91.5</td><td>84.5</td><td>46.8</td></tr>
                    </tbody>
                    <caption class="is-size-7" style="caption-side: bottom; text-align: center; margin-top: 0.5rem;">Table 2 from the paper: Performance comparison on mathematical reasoning benchmarks.</caption>
                </table>
            </div>
        </div>
    </div>
    
    <div class="columns is-centered" style="margin-top: 2rem;">
        <div class="column is-four-fifths">
            <h3 class="title is-4 has-text-centered">Ablation on Reward Modeling</h3>
            <article class="message is-info">
              <div class="message-body">
                To find how to effectively stimulate parallel thinking, we tested several reward strategies. We found that an <strong>alternating reward</strong> provides the best balance between a high parallel ratio (63.0%) and strong performance, achieving the best results on challenging benchmarks like AIME25.
              </div>
            </article>
            <table class="table is-bordered is-fullwidth results-table">
                <thead>
                    <tr><th>Training Configuration</th><th>Parallel Ratio (%)</th><th>AIME 25</th><th>AIME 24</th><th>AMC 23</th><th>MATH</th></tr>
                </thead>
                <tbody>
                    <tr><td>Accuracy Only</td><td>13.6</td><td>17.7</td><td>18.3</td><td>69.7</td><td>82.6</td></tr>
                    <tr><td>Parallel Only</td><td>80.3</td><td>17.7</td><td>15.2</td><td>59.4</td><td>81.7</td></tr>
                    <tr style="background-color: #e9f5ff;"><td><strong>Alternating Acc./Parallel</strong></td><td class="highlight-best">63.0</td><td class="highlight-best">19.0</td><td>16.3</td><td>67.5</td><td class="highlight-best">84.5</td></tr>
                </tbody>
                <caption class="is-size-7" style="caption-side: bottom; text-align: center; margin-top: 0.5rem;">Table 4 from the paper: Ablation study on reward modeling for the Parallel-R1-Unseen model.</caption>
            </table>
        </div>
    </div>

    <div class="columns is-centered">
        <div class="column is-four-fifths">
            <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Finding: Evolution of Behavior</h3>
        </div>
    </div>
    <div class="columns is-centered is-vcentered">
        <div class="column is-half">
            <article class="message is-info">
              <div class="message-body">
                Our analysis reveals a clear strategic evolution: the model initially leverages parallel paths for <strong>exploration</strong>, but as it gains proficiency, its strategy shifts towards using them for <strong>verification</strong>. This is shown by the relative position of the parallel thinking block moving later in the generation process as training progresses.
              </div>
            </article>
             <button class="button is-info is-outlined is-small" id="toggle-cases-btn" style="margin-top: 1rem;">Show Supporting Case Studies</button>
        </div>
        <div class="column is-half">
            <img src="./static/images/parallel_position.jpg" alt="Graph showing the relative position of the parallel block during RL training">
            <p class="is-size-7 has-text-centered" style="margin-top: 0.5rem;">Figure 3 from the paper: Positional dynamics of the <code>&lt;Parallel&gt;</code> block.</p>
        </div>
    </div>

    <div id="case-studies-collapsible" class="collapsible-content">
        <div class="columns is-centered">
            <div class="column">
                <div class="case-study-container">
                    <div class="case-study">
                        <h5 class="title is-5 has-text-centered">Early Stage: Exploration</h5>
                        <img src="./static/images/case1.jpg" alt="Case study of an early-stage model.">
                    </div>
                    <div class="case-study">
                        <h5 class="title is-5 has-text-centered">Late Stage: Verification</h5>
                        <img src="./static/images/case2.jpg" alt="Case study of a late-stage model.">
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="columns is-centered">
        <div class="column is-four-fifths">
            <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Finding: Parallel Thinking as a Mid-Training Scaffold</h3>
        </div>
    </div>
    <div class="columns is-centered is-vcentered">
        <div class="column is-half">
            <article class="message is-info">
                <div class="message-body">
                    We found that parallel thinking can serve as a structured exploration "scaffold". By temporarily forcing the model to explore with parallel paths, we guide it toward more robust policy spaces. When we later switch to optimizing for accuracy alone, the model's performance improves significantly, reaching a peak AIME25 accuracy of 25.6%, surpassing the baseline.
                </div>
            </article>
        </div>
        <div class="column is-half">
            <img src="./static/images/mid_training.jpg" alt="Graph showing two-stage training with parallel reasoning as a scaffold">
            <p class="is-size-7 has-text-centered" style="margin-top: 0.5rem;">Figure 4 from the paper.</p>
        </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zheng2025parallelr1,
      title={Parallel-R1: Towards Parallel Thinking via Reinforcement Learning}, 
      author={Tong Zheng and Hongming Zhang and Wenhao Yu and Xiaoyang Wang and Xinyu Yang and Runpeng Dai and Rui Liu and Huiwen Bao and Chengsong Huang and Heng Huang and Dong Yu},
      year={2025},
      eprint={your-arxiv-id},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage template was borrowed from the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  // Script for the collapsible case studies
  document.addEventListener('DOMContentLoaded', () => {
    const toggleBtn = document.getElementById('toggle-cases-btn');
    const content = document.getElementById('case-studies-collapsible');

    if (toggleBtn && content) {
      toggleBtn.addEventListener('click', () => {
        content.classList.toggle('is-active');
        if (content.classList.contains('is-active')) {
          toggleBtn.textContent = 'Hide Supporting Case Studies';
        } else {
          toggleBtn.textContent = 'Show Supporting Case Studies';
        }
      });
    }
  });
</script>

</body>
</html>
