<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Parallel-R1: Towards Parallel Thinking via Reinforcement Learning">
  <meta name="keywords" content="Parallel Thinking, Reinforcement Learning, LLM, Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  
  <style>
    .hero.teaser .container {
      max-width: 100%;
    }
    .publication-title {
      font-family: 'Google Sans', sans-serif;
      font-weight: 500;
    }
    .publication-authors {
      font-family: 'Google Sans', sans-serif;
    }
    .publication-venue {
        color: #555;
        font-size: 1rem;
        font-weight: 500;
    }
    .link-block a {
      color: #3273dc;
    }
    .section {
      padding: 3rem 1.5rem;
    }
    .content h1, .content h2, .content h3, .content h4, .content h5 {
        font-family: 'Google Sans', sans-serif;
        font-weight: 500;
    }
    .publication-links .button {
        font-weight: 500;
    }
    .results-table {
        width: 100%;
        border-collapse: collapse;
        margin: 1.5rem 0;
        font-size: 0.85em;
    }
    .results-table th, .results-table td {
        border: 1px solid #dbdbdb;
        padding: 0.7em;
        vertical-align: middle;
        white-space: nowrap;
    }
    .results-table thead {
        background-color: #f5f5f5;
        font-weight: bold;
    }
    .highlight-best {
        font-weight: bold;
        color: #257942;
    }
    .dnerf {
      color: #3273dc;
    }
    .case-study-container {
        display: flex;
        gap: 2rem;
    }
    .case-study {
        flex: 1;
    }
    .case-study img {
        border: 1px solid #ddd;
        border-radius: 5px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    .collapsible-content {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.5s ease-in-out, margin-top 0.5s ease-in-out;
        margin-top: 0 !important;
    }
    .collapsible-content.is-active {
        max-height: 1000px;
        margin-top: 2rem !important;
    }
    #highlights .box {
      background-color: #f0f8ff;
      border-left: 4px solid #3273dc;
    }
    #highlights .title {
      color: #3273dc;
    }
    @media screen and (max-width: 768px) {
        .case-study-container {
            flex-direction: column;
        }
    }
  </style>
  
  <link rel="icon" href="data:;base64,iVBORw0KGgo=">

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</h1>
          <h3 class="subtitle is-4 publication-venue">Technical Report, 2025</h3>
          <div class="is-size-5 publication-authors">
             <span class="author-block">
              <a href="#">Tong Zheng</a><sup>1,2,†</sup>,</span>
            <span class="author-block">
              <a href="#">Hongming Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Wenhao Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Xiaoyang Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Xinyu Yang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="#">Runpeng Dai</a><sup>1,4</sup>,
            </span><br>
            <span class="author-block">
              <a href="#">Rui Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Huiwen Bao</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="#">Chengsong Huang</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="#">Heng Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Dong Yu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tencent AI Lab Seattle,</span>
            <span class="author-block"><sup>2</sup>University of Maryland, College Park,</span>
            <span class="author-block"><sup>3</sup>Carnegie Mellon University,</span><br>
            <span class="author-block"><sup>4</sup>University of North Carolina at Chapel Hill,</span>
            <span class="author-block"><sup>5</sup>City University of Hong Kong,</span>
            <span class="author-block"><sup>6</sup>Washington University in St. Louis</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="your_paper_link.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/your-arxiv-id"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg style="width:18px;height:18px;margin-top:-2px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg"><path d="M512 0C229.2224 0 0 229.2224 0 512s229.2224 512 512 512 512-229.2224 512-512S794.7776 0 512 0z m233.472 696.32L623.36 574.336l-92.16 122.368-124.416-165.888-122.368 165.888L162.816 328.192h142.336l91.648 122.368 124.416 165.888 122.368-165.888 91.136-122.368h142.336L745.472 696.32z" fill="#FFFFFF"></path></svg>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/zhengkid/Parallel-R1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./static/images/Parallel_Thinking_illustration.jpg" alt="An overview of the Parallel-R1 framework" style="width: 100%; border-radius: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.15);">
      <h2 class="subtitle has-text-centered" style="margin-top: 2rem;">
        <span class="dnerf">Parallel-R1</span> teaches Large Language Models to learn parallel thinking through reinforcement learning, turning a single line of thought into a multi-path reasoning strategy.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization.
          </p>
          <p>
            Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. 
          </p>
        </div>
      </div>
    </div>
    
    <div id="highlights" class="columns is-centered" style="margin-top: 2rem;">
        <div class="column is-four-fifths">
            <div class="box">
                <h3 class="title is-4 has-text-centered">Highlights</h3>
                <div class="content">
                    <ul>
                        <li>Achieves an **8.4%** accuracy improvement over sequential thinking models trained with RL.</li>
                        <li>Demonstrates that parallel thinking can be used as a mid-training exploration scaffold, yielding a **42.9%** improvement over the baseline[cite: 14].</li>
                        <li>Reaches a peak accuracy of **25.6%** on the challenging AIME25 benchmark[cite: 80].</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
  </div>
</section>


<section class="section" style="background-color: #f9f9f9;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Challenges in Teaching Parallel Thinking</h2>
        <div class="content has-text-justified">
            <p>
                While powerful, teaching LLMs to think in parallel via Reinforcement Learning is non-trivial and presents several core challenges that our work aims to address.
            </p>
        </div>
      </div>
    </div>
    
    <div class="columns is-centered">
        <div class="column is-four-fifths">
            <article class="message is-info">
              <div class="message-header"><p>1. The RL Cold-Start & Data Bottleneck</p></div>
              <div class="message-body">
                LLMs pretrained on sequential text have a "cold-start" problem—they never generate parallel structures, so RL has nothing to reward[cite: 58]. Furthermore, collecting high-quality parallel thinking data is a major challenge, as it's rare in natural text.
              </div>
            </article>
            <article class="message is-info">
              <div class="message-header"><p>2. The Reward Design Dilemma</p></div>
              <div class="message-body">
                The ideal reward function is unclear[cite: 61]. If we only reward final accuracy, the model might learn to "cheat" and abandon complex parallel thinking. If we only reward the use of parallel structures, performance on the actual task might suffer[cite: 61, 62].
              </div>
            </article>
            <article class="message is-info">
              <div class="message-header"><p>3. The "Black Box" Strategy</p></div>
              <div class="message-body">
                Even if a model learns this skill, its strategic role and underlying mechanisms are a "black box"[cite: 63]. How does the model's strategy evolve during training? Without understanding this dynamic, it's impossible to fully unlock the potential of parallel thinking[cite: 65, 66].
              </div>
            </article>
        </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Our Contributions</h2>
            </div>
        </div>
        
        <div class="columns is-centered">
            <div class="column is-four-fifths">
                <h3 class="title is-4 has-text-centered">1. The Parallel-R1 Framework</h3>
                <div class="content has-text-justified">
                  <p>
                    To solve the cold-start and reward design challenges, we propose a complete framework featuring a progressive curriculum and dedicated reward design.
                  </p>
                  <ul>
                    <li><strong>Progressive Curriculum</strong>: We leverage a key finding that it's easy to generate parallel data for simple tasks (like GSM8K). We create the <strong>Parallel-GSM8K</strong> dataset to first teach the model the *format* of parallel thinking via SFT, before using RL to generalize the skill on harder problems.</li>
                    <li><strong>Dedicated Reward Design</strong>: We propose an <strong>alternating reward strategy</strong> that switches between an accuracy-only reward and a tiered reward that gives a bonus to correct answers generated with parallel thinking.</li>
                  </ul>
                </div>
            </div>
        </div>

        <div class="columns is-centered">
            <div class="column is-four-fifths">
                <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">2. Uncovering Learning Dynamics</h3>
            </div>
        </div>
        <div class="columns is-centered is-vcentered">
            <div class="column is-half">
                <div class="content has-text-justified">
                    <p>
                      To open the "black box", we provide the first empirical evidence of how an LLM's reasoning strategy with parallel thinking evolves[cite: 79]. Our analysis reveals a clear strategic shift: the model initially leverages parallel paths for <strong>exploration</strong>, but as it gains proficiency, it shifts towards using them for <strong>verification</strong>.
                    </p>
                </div>
                 <button class="button is-info is-outlined is-small" id="toggle-cases-btn">Show Supporting Case Studies</button>
            </div>
            <div class="column is-half">
                <img src="./static/images/parallel_position.jpg" alt="Graph showing the relative position of the parallel block during RL training">
            </div>
        </div>

        <div id="case-studies-collapsible" class="collapsible-content">
            <div class="columns is-centered">
                <div class="column">
                    <div class="case-study-container">
                        <div class="case-study">
                            <h5 class="title is-5 has-text-centered">Early Stage: Exploration</h5>
                            <img src="./static/images/case1.jpg" alt="Case study of an early-stage model.">
                        </div>
                        <div class="case-study">
                            <h5 class="title is-5 has-text-centered">Late Stage: Verification</h5>
                            <img src="./static/images/case2.jpg" alt="Case study of a late-stage model.">
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="columns is-centered">
            <div class="column is-four-fifths">
                <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">3. Parallel Thinking as a Mid-Training Scaffold</h3>
            </div>
        </div>
        <div class="columns is-centered is-vcentered">
            <div class="column is-half">
                <div class="content has-text-justified">
                    <p>
                        We conceptualize and validate that parallel thinking can serve as a structured exploration "scaffold"[cite: 80]. By temporarily forcing the model to explore with parallel paths, we guide it toward more robust policy spaces. This temporary phase unlocks a higher performance ceiling after RL, yielding a **42.9% improvement** over the baseline and reaching a peak accuracy of **25.6% on AIME25**.
                    </p>
                </div>
            </div>
            <div class="column is-half">
                <img src="./static/images/mid_training.jpg" alt="Graph showing two-stage training with parallel reasoning as a scaffold">
            </div>
        </div>
    </div>
</section>

<section class="section" style="background-color: #f9f9f9;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Main Results at a Glance</h2>
        </div>
      </div>

    <div class="columns is-centered">
        <div class="column is-four-fifths">
            <article class="message is-info">
              <div class="message-body">
                Our full framework leads to an **8.4% average accuracy improvement** over the sequential thinking model trained directly on challenging tasks with RL. The table below provides a detailed breakdown of performance across all benchmarks and configurations.
              </div>
            </article>
            <div style="overflow-x: auto;">
                <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth results-table">
                    <thead>
                        <tr>
                            <th rowspan="2">Method</th> <th rowspan="2"># Parallel (%)</th> <th colspan="2">AIME25</th> <th colspan="2">AIME24</th> <th colspan="2">AMC23</th> <th rowspan="2">MATH (Mean@1)</th> <th rowspan="2">Avg.</th>
                        </tr>
                        <tr>
                            <th>Mean@16</th> <th>Pass@16</th> <th>Mean@16</th> <th>Pass@16</th> <th>Mean@16</th> <th>Pass@16</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>Qwen3-4B-Base</td><td>0.0</td><td>1.3</td><td>10.2</td><td>2.9</td><td>16.5</td><td>8.1</td><td>51.2</td><td>13.9</td><td>6.6</td></tr>
                        <tr style="background-color: #fafafa;"><td colspan="10" style="font-style: italic; text-align: left;"><strong>SFT + Parallel</strong></td></tr>
                        <tr><td>Parallel-SFT-Seen</td><td>95.6</td><td>8.0</td><td>29.8</td><td>10.6</td><td>26.4</td><td>48.9</td><td>79.2</td><td>76.6</td><td>36.0</td></tr>
                        <tr><td>Parallel-SFT-Unseen</td><td>95.6</td><td>5.2</td><td>20.9</td><td>8.5</td><td>26.7</td><td>41.7</td><td>80.1</td><td>71.5</td><td>31.7</td></tr>
                        <tr style="background-color: #fafafa;"><td colspan="10" style="font-style: italic; text-align: left;"><strong>RL Approach</strong></td></tr>
                        <tr><td>GRPO (DAPO)</td><td>0.0</td><td>14.8</td><td>32.4</td><td>18.5</td><td>30.6</td><td>63.6</td><td>85.1</td><td>83.5</td><td>45.1</td></tr>
                        <tr><td>&nbsp;&nbsp; + RL on GSM8K</td><td>0.0</td><td>13.3</td><td>26.3</td><td>18.8</td><td>34.9</td><td>66.4</td><td>82.2</td><td>82.6</td><td>45.3</td></tr>
                        <tr style="background-color: #e9f5ff;"><td>Parallel-R1-Seen</td><td>27.3</td><td class="highlight-best">19.2</td><td>38.9</td><td class="highlight-best">19.4</td><td class="highlight-best">37.1</td><td class="highlight-best">70.5</td><td>85.0</td><td class="highlight-best">86.7</td><td class="highlight-best">48.9</td></tr>
                        <tr><td>Parallel-R1-Unseen (S1)</td><td>13.6</td><td>17.7</td><td>37.8</td><td>18.3</td><td>33.2</td><td>69.7</td><td>88.9</td><td>82.6</td><td>47.1</td></tr>
                        <tr><td>Parallel-R1-Unseen (S2)</td><td class="highlight-best">63.0</td><td>19.0</td><td class="highlight-best">42.2</td><td>16.3</td><td>31.8</td><td>67.5</td><td class="highlight-best">91.5</td><td>84.5</td><td>46.8</td></tr>
                    </tbody>
                    <caption class="is-size-7" style="caption-side: bottom; text-align: center; margin-top: 0.5rem;">Table 2 from the paper: Performance comparison on mathematical reasoning benchmarks.</caption>
                </table>
            </div>
        </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zheng2025parallelr1,
      title={Parallel-R1: Towards Parallel Thinking via Reinforcement Learning}, 
      author={Tong Zheng and Hongming Zhang and Wenhao Yu and Xiaoyang Wang and Xinyu Yang and Runpeng Dai and Rui Liu and Huiwen Bao and Chengsong Huang and Heng Huang and Dong Yu},
      year={2025},
      eprint={your-arxiv-id},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage template was borrowed from the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  // Script for the collapsible case studies
  document.addEventListener('DOMContentLoaded', () => {
    const toggleBtn = document.getElementById('toggle-cases-btn');
    const content = document.getElementById('case-studies-collapsible');

    if (toggleBtn && content) {
      toggleBtn.addEventListener('click', () => {
        content.classList.toggle('is-active');
        if (content.classList.contains('is-active')) {
          toggleBtn.textContent = 'Hide Supporting Case Studies';
        } else {
          toggleBtn.textContent = 'Show Supporting Case Studies';
        }
      });
    }
  });
</script>

</body>
</html>
